{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicoleolivetto/MMD_Final_Project/blob/main/MMDS_VR481171.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM"
      ],
      "metadata": {
        "id": "VoAf7C2CKUDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the data\n",
        "imdb_data = pd.read_csv('IMDB Dataset.csv')\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Data preprocessing\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopwords_list = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    # Tokenize\n",
        "    tokens = tokenizer.tokenize(text.lower())\n",
        "    # Remove stopwords\n",
        "    tokens = [token for token in tokens if token not in stopwords_list]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "imdb_data['clean_review'] = imdb_data['review'].apply(preprocess_text)\n",
        "\n",
        "# Tokenization and padding\n",
        "max_length = 100\n",
        "vocab_size = 8000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(imdb_data['clean_review'])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(imdb_data['clean_review'])\n",
        "X = pad_sequences(sequences, maxlen=max_length)\n",
        "y = np.array(imdb_data['sentiment'].map({'positive': 1, 'negative': 0}))\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Model definition\n",
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "model.add(LSTM(units=200))\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model training\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=5, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Model evaluation\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Evaluation metrics\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7OklZJIJm5Z",
        "outputId": "659d3180-1b64-4ba0-f3b4-579ec83376be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "<ipython-input-2-04960a3fa411>:27: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "219/219 [==============================] - 35s 138ms/step - loss: 0.4082 - accuracy: 0.8094 - val_loss: 0.3136 - val_accuracy: 0.8681\n",
            "Epoch 2/5\n",
            "219/219 [==============================] - 16s 71ms/step - loss: 0.2414 - accuracy: 0.9075 - val_loss: 0.2920 - val_accuracy: 0.8780\n",
            "Epoch 3/5\n",
            "219/219 [==============================] - 10s 48ms/step - loss: 0.1842 - accuracy: 0.9320 - val_loss: 0.3206 - val_accuracy: 0.8684\n",
            "Epoch 4/5\n",
            "219/219 [==============================] - 7s 32ms/step - loss: 0.1300 - accuracy: 0.9527 - val_loss: 0.5006 - val_accuracy: 0.8623\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.2983 - accuracy: 0.8777\n",
            "Test Loss: 0.298311710357666, Test Accuracy: 0.8776666522026062\n",
            "469/469 [==============================] - 2s 3ms/step\n",
            "Accuracy: 0.8776666666666667\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.88      0.88      7411\n",
            "           1       0.88      0.88      0.88      7589\n",
            "\n",
            "    accuracy                           0.88     15000\n",
            "   macro avg       0.88      0.88      0.88     15000\n",
            "weighted avg       0.88      0.88      0.88     15000\n",
            "\n",
            "[[6514  897]\n",
            " [ 938 6651]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "D1yKWqmoKFJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
        "\n",
        "# Define the CNN model\n",
        "model_cnn = Sequential()\n",
        "\n",
        "# Embedding layer\n",
        "model_cnn.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=100))\n",
        "\n",
        "# First convolutional layer\n",
        "model_cnn.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "model_cnn.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Second convolutional layer\n",
        "model_cnn.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model_cnn.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Flatten the output of the convolutional layers\n",
        "model_cnn.add(Flatten())\n",
        "\n",
        "# Fully connected layer\n",
        "model_cnn.add(Dense(units=64, activation='relu'))\n",
        "\n",
        "# Dropout layer\n",
        "model_cnn.add(Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model_cnn.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model_cnn.summary()\n",
        "\n",
        "\n",
        "# Training CNN model\n",
        "history_cnn = model_cnn.fit(X_train, y_train, epochs=8, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluation CNN model\n",
        "loss_cnn, accuracy_cnn = model_cnn.evaluate(X_test, y_test)\n",
        "print(f'CNN Test Loss: {loss_cnn}, CNN Test Accuracy: {accuracy_cnn}')\n",
        "y_pred_cnn_prob = model_cnn.predict(X_test)\n",
        "y_pred_cnn = (y_pred_cnn_prob > 0.5).astype(int)\n",
        "\n",
        "# Evaluation metrics for CNN\n",
        "print(\"CNN Model Accuracy:\", accuracy_score(y_test, y_pred_cnn))\n",
        "print(\"CNN Model Classification Report:\\n\", classification_report(y_test, y_pred_cnn))\n",
        "print(\"CNN Model Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_cnn))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V38srzBKGe8",
        "outputId": "174fb3f4-cd26-427a-f653-c06b13b62bac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 100, 100)          800000    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 98, 128)           38528     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 49, 128)           0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 47, 64)            24640     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 23, 64)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1472)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                94272     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 957505 (3.65 MB)\n",
            "Trainable params: 957505 (3.65 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/8\n",
            "219/219 [==============================] - 23s 93ms/step - loss: 0.4531 - accuracy: 0.7599 - val_loss: 0.3008 - val_accuracy: 0.8741\n",
            "Epoch 2/8\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.2450 - accuracy: 0.9057 - val_loss: 0.3116 - val_accuracy: 0.8734\n",
            "Epoch 3/8\n",
            "219/219 [==============================] - 6s 27ms/step - loss: 0.1694 - accuracy: 0.9381 - val_loss: 0.3455 - val_accuracy: 0.8699\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3022 - accuracy: 0.8736\n",
            "CNN Test Loss: 0.302228718996048, CNN Test Accuracy: 0.8736000061035156\n",
            "469/469 [==============================] - 1s 2ms/step\n",
            "CNN Model Accuracy: 0.8736\n",
            "CNN Model Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.86      0.87      7411\n",
            "           1       0.87      0.89      0.88      7589\n",
            "\n",
            "    accuracy                           0.87     15000\n",
            "   macro avg       0.87      0.87      0.87     15000\n",
            "weighted avg       0.87      0.87      0.87     15000\n",
            "\n",
            "CNN Model Confusion Matrix:\n",
            " [[6366 1045]\n",
            " [ 851 6738]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM-CNN"
      ],
      "metadata": {
        "id": "C_gPLIdZKPnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM, Bidirectional\n",
        "\n",
        "# Define the LSTM-CNN model\n",
        "model_lstm_cnn = Sequential()\n",
        "\n",
        "# Embedding layer (assuming you already have it defined)\n",
        "model_lstm_cnn.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "\n",
        "# LSTM layer\n",
        "model_lstm_cnn.add(Bidirectional(LSTM(units=200, return_sequences=True)))\n",
        "\n",
        "# Convolutional layers\n",
        "model_lstm_cnn.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "model_lstm_cnn.add(MaxPooling1D(pool_size=2))\n",
        "model_lstm_cnn.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model_lstm_cnn.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Flatten the output of the convolutional layers\n",
        "model_lstm_cnn.add(Flatten())\n",
        "\n",
        "# Fully connected layer\n",
        "model_lstm_cnn.add(Dense(units=64, activation='relu'))\n",
        "\n",
        "# Dropout layer\n",
        "model_lstm_cnn.add(Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model_lstm_cnn.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_lstm_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model_lstm_cnn.summary()\n",
        "\n",
        "# Training LSTM-CNN model\n",
        "history_lstm_cnn = model_lstm_cnn.fit(X_train, y_train, epochs=6, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluation LSTM-CNN model\n",
        "loss_lstm_cnn, accuracy_lstm_cnn = model_lstm_cnn.evaluate(X_test, y_test)\n",
        "print(f'LSTM-CNN Test Loss: {loss_lstm_cnn}, LSTM-CNN Test Accuracy: {accuracy_lstm_cnn}')\n",
        "y_pred_lstm_cnn_prob = model_lstm_cnn.predict(X_test)\n",
        "y_pred_lstm_cnn = (y_pred_lstm_cnn_prob > 0.5).astype(int)\n",
        "\n",
        "# Evaluation metrics for LSTM-CNN\n",
        "print(\"LSTM-CNN Model Accuracy:\", accuracy_score(y_test, y_pred_lstm_cnn))\n",
        "print(\"LSTM-CNN Model Classification Report:\\n\", classification_report(y_test, y_pred_lstm_cnn))\n",
        "print(\"LSTM-CNN Model Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lstm_cnn))\n"
      ],
      "metadata": {
        "id": "FNMcJQRnKLD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STRESS-TEST\n",
        "1. NOISE"
      ],
      "metadata": {
        "id": "SlvaO09_KZDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding noise\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the data\n",
        "imdb_data = pd.read_csv('IMDB Dataset.csv')\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Data preprocessing\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopwords_list = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    # Tokenize\n",
        "    tokens = tokenizer.tokenize(text.lower())\n",
        "    # Remove stopwords\n",
        "    tokens = [token for token in tokens if token not in stopwords_list]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "imdb_data['clean_review'] = imdb_data['review'].apply(preprocess_text)\n",
        "\n",
        "# Add Gaussian noise to word embeddings\n",
        "def add_noise_to_embeddings(embeddings, noise_factor):\n",
        "    noise = np.random.normal(loc=0.0, scale=noise_factor, size=embeddings.shape)\n",
        "    noisy_embeddings = embeddings + noise\n",
        "    return noisy_embeddings\n",
        "\n",
        "# Tokenization and padding\n",
        "max_length = 100\n",
        "vocab_size = 8000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(imdb_data['clean_review'])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(imdb_data['clean_review'])\n",
        "X = pad_sequences(sequences, maxlen=max_length)\n",
        "y = np.array(imdb_data['sentiment'].map({'positive': 1, 'negative': 0}))\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Add noise to word embeddings\n",
        "noise_factor = 1  # Adjust the noise factor as desired\n",
        "noisy_X_train = add_noise_to_embeddings(X_train, noise_factor)\n",
        "\n",
        "\n",
        "# Model definition\n",
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "model.add(LSTM(units=200))\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model training\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(noisy_X_train, y_train, epochs=5, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Model evaluation\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Evaluation metrics\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# CNN Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
        "\n",
        "# Define the CNN model\n",
        "model_cnn = Sequential()\n",
        "\n",
        "# Embedding layer\n",
        "model_cnn.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=100))\n",
        "\n",
        "# First convolutional layer\n",
        "model_cnn.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "model_cnn.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Second convolutional layer\n",
        "model_cnn.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model_cnn.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Flatten the output of the convolutional layers\n",
        "model_cnn.add(Flatten())\n",
        "\n",
        "# Fully connected layer\n",
        "model_cnn.add(Dense(units=64, activation='relu'))\n",
        "\n",
        "# Dropout layer\n",
        "model_cnn.add(Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model_cnn.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model_cnn.summary()\n",
        "\n",
        "\n",
        "# Training CNN model\n",
        "history_cnn = model_cnn.fit(noisy_X_train, y_train, epochs=8, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluation CNN model\n",
        "loss_cnn, accuracy_cnn = model_cnn.evaluate(X_test, y_test)\n",
        "print(f'CNN Test Loss: {loss_cnn}, CNN Test Accuracy: {accuracy_cnn}')\n",
        "y_pred_cnn_prob = model_cnn.predict(X_test)\n",
        "y_pred_cnn = (y_pred_cnn_prob > 0.5).astype(int)\n",
        "\n",
        "# Evaluation metrics for CNN\n",
        "print(\"CNN Model Accuracy:\", accuracy_score(y_test, y_pred_cnn))\n",
        "print(\"CNN Model Classification Report:\\n\", classification_report(y_test, y_pred_cnn))\n",
        "print(\"CNN Model Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_cnn))\n",
        "\n",
        "from keras.layers import LSTM, Bidirectional\n",
        "\n",
        "# Define the LSTM-CNN model\n",
        "model_lstm_cnn = Sequential()\n",
        "\n",
        "# Embedding layer (assuming you already have it defined)\n",
        "model_lstm_cnn.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "\n",
        "# LSTM layer\n",
        "model_lstm_cnn.add(Bidirectional(LSTM(units=200, return_sequences=True)))\n",
        "\n",
        "# Convolutional layers\n",
        "model_lstm_cnn.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "model_lstm_cnn.add(MaxPooling1D(pool_size=2))\n",
        "model_lstm_cnn.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model_lstm_cnn.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Flatten the output of the convolutional layers\n",
        "model_lstm_cnn.add(Flatten())\n",
        "\n",
        "# Fully connected layer\n",
        "model_lstm_cnn.add(Dense(units=64, activation='relu'))\n",
        "\n",
        "# Dropout layer\n",
        "model_lstm_cnn.add(Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model_lstm_cnn.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_lstm_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model_lstm_cnn.summary()\n",
        "\n",
        "# Training LSTM-CNN model\n",
        "history_lstm_cnn = model_lstm_cnn.fit(noisy_X_train, y_train, epochs=6, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluation LSTM-CNN model\n",
        "loss_lstm_cnn, accuracy_lstm_cnn = model_lstm_cnn.evaluate(X_test, y_test)\n",
        "print(f'LSTM-CNN Test Loss: {loss_lstm_cnn}, LSTM-CNN Test Accuracy: {accuracy_lstm_cnn}')\n",
        "y_pred_lstm_cnn_prob = model_lstm_cnn.predict(X_test)\n",
        "y_pred_lstm_cnn = (y_pred_lstm_cnn_prob > 0.5).astype(int)\n",
        "\n",
        "# Evaluation metrics for LSTM-CNN\n",
        "print(\"LSTM-CNN Model Accuracy:\", accuracy_score(y_test, y_pred_lstm_cnn))\n",
        "print(\"LSTM-CNN Model Classification Report:\\n\", classification_report(y_test, y_pred_lstm_cnn))\n",
        "print(\"LSTM-CNN Model Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lstm_cnn))\n"
      ],
      "metadata": {
        "id": "WXLRN3XWKg_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. IMBALANCED DATASET"
      ],
      "metadata": {
        "id": "2qqPqE45Rpd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, Bidirectional\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the data\n",
        "imdb_data = pd.read_csv('IMDB Dataset.csv')\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Data preprocessing\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopwords_list = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    # Tokenize\n",
        "    tokens = tokenizer.tokenize(text.lower())\n",
        "    # Remove stopwords\n",
        "    tokens = [token for token in tokens if token not in stopwords_list]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "imdb_data['clean_review'] = imdb_data['review'].apply(preprocess_text)\n",
        "\n",
        "# Filter positive and negative reviews\n",
        "positive_reviews = imdb_data[imdb_data['sentiment'] == 'positive'].sample(n=15000, random_state=42)\n",
        "negative_reviews = imdb_data[imdb_data['sentiment'] == 'negative'].sample(n=10000, random_state=42)\n",
        "\n",
        "# Concatenate positive and negative reviews to create imbalanced dataset\n",
        "imbalanced_data = pd.concat([positive_reviews, negative_reviews], ignore_index=True)\n",
        "\n",
        "# Tokenization and padding\n",
        "max_length = 100\n",
        "vocab_size = 8000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(imbalanced_data['clean_review'])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(imbalanced_data['clean_review'])\n",
        "X = pad_sequences(sequences, maxlen=max_length)\n",
        "y = np.array(imbalanced_data['sentiment'].map({'positive': 1, 'negative': 0}))\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Model definitions\n",
        "embedding_dim = 100\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "# LSTM Model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "lstm_model.add(LSTM(units=200))\n",
        "lstm_model.add(Dense(units=64, activation='relu'))\n",
        "lstm_model.add(Dropout(0.5))\n",
        "lstm_model.add(Dense(units=1, activation='sigmoid'))\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# CNN Model\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_length))\n",
        "cnn_model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "cnn_model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(units=64, activation='relu'))\n",
        "cnn_model.add(Dropout(0.5))\n",
        "cnn_model.add(Dense(units=1, activation='sigmoid'))\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# LSTM-CNN Model\n",
        "lstm_cnn_model = Sequential()\n",
        "lstm_cnn_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "lstm_cnn_model.add(Bidirectional(LSTM(units=200, return_sequences=True)))\n",
        "lstm_cnn_model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "lstm_cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "lstm_cnn_model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "lstm_cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "lstm_cnn_model.add(Flatten())\n",
        "lstm_cnn_model.add(Dense(units=64, activation='relu'))\n",
        "lstm_cnn_model.add(Dropout(0.5))\n",
        "lstm_cnn_model.add(Dense(units=1, activation='sigmoid'))\n",
        "lstm_cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train models\n",
        "lstm_history = lstm_model.fit(X_train, y_train, epochs=5, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
        "cnn_history = cnn_model.fit(X_train, y_train, epochs=8, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
        "lstm_cnn_history = lstm_cnn_model.fit(X_train, y_train, epochs=6, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate models\n",
        "loss_lstm, accuracy_lstm = lstm_model.evaluate(X_test, y_test)\n",
        "loss_cnn, accuracy_cnn = cnn_model.evaluate(X_test, y_test)\n",
        "loss_lstm_cnn, accuracy_lstm_cnn = lstm_cnn_model.evaluate(X_test, y_test)\n",
        "\n",
        "print(f'LSTM Test Loss: {loss_lstm}, LSTM Test Accuracy: {accuracy_lstm}')\n",
        "print(f'CNN Test Loss: {loss_cnn}, CNN Test Accuracy: {accuracy_cnn}')\n",
        "print(f'LSTM-CNN Test Loss: {loss_lstm_cnn}, LSTM-CNN Test Accuracy: {accuracy_lstm_cnn}')\n"
      ],
      "metadata": {
        "id": "M9lk_DugN4XR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics for LSTM\n",
        "y_pred_lstm = (lstm_model.predict(X_test) > 0.5).astype(int)\n",
        "print(\"LSTM Model Accuracy:\", accuracy_score(y_test, y_pred_lstm))\n",
        "print(\"LSTM Model Classification Report:\\n\", classification_report(y_test, y_pred_lstm))\n",
        "print(\"LSTM Model Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lstm))\n",
        "\n",
        "# Evaluation metrics for CNN\n",
        "y_pred_cnn = (cnn_model.predict(X_test) > 0.5).astype(int)\n",
        "print(\"CNN Model Accuracy:\", accuracy_score(y_test, y_pred_cnn))\n",
        "print(\"CNN Model Classification Report:\\n\", classification_report(y_test, y_pred_cnn))\n",
        "print(\"CNN Model Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_cnn))\n",
        "\n",
        "# Evaluation metrics for LSTM-CNN\n",
        "y_pred_lstm_cnn = (lstm_cnn_model.predict(X_test) > 0.5).astype(int)\n",
        "print(\"LSTM-CNN Model Accuracy:\", accuracy_score(y_test, y_pred_lstm_cnn))\n",
        "print(\"LSTM-CNN Model Classification Report:\\n\", classification_report(y_test, y_pred_lstm_cnn))\n",
        "print(\"LSTM-CNN Model Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lstm_cnn))"
      ],
      "metadata": {
        "id": "MfpkBb-JN4ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.BATCH SIZE"
      ],
      "metadata": {
        "id": "um8cxBczN-K4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8,24,512 batch size\n",
        "\n",
        "#batch size\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Bidirectional\n",
        "\n",
        "# Load the data\n",
        "imdb_data = pd.read_csv('IMDB Dataset.csv')\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Data preprocessing\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopwords_list = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    # Tokenize\n",
        "    tokens = tokenizer.tokenize(text.lower())\n",
        "    # Remove stopwords\n",
        "    tokens = [token for token in tokens if token not in stopwords_list]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "imdb_data['clean_review'] = imdb_data['review'].apply(preprocess_text)\n",
        "\n",
        "# Tokenization and padding\n",
        "max_length = 100\n",
        "vocab_size = 8000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(imdb_data['clean_review'])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(imdb_data['clean_review'])\n",
        "X = pad_sequences(sequences, maxlen=max_length)\n",
        "y = np.array(imdb_data['sentiment'].map({'positive': 1, 'negative': 0}))\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Model definition - LSTM\n",
        "embedding_dim = 100\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "model_lstm.add(LSTM(units=200))\n",
        "model_lstm.add(Dense(units=64, activation='relu'))\n",
        "model_lstm.add(Dropout(0.5))\n",
        "model_lstm.add(Dense(units=1, activation='sigmoid'))\n",
        "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model training - LSTM\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "history_lstm = model_lstm.fit(X_train, y_train, epochs=5, batch_size=512, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Model evaluation - LSTM\n",
        "loss_lstm, accuracy_lstm = model_lstm.evaluate(X_test, y_test)\n",
        "print(f'LSTM Test Loss: {loss_lstm}, LSTM Test Accuracy: {accuracy_lstm}')\n",
        "y_pred_lstm = (model_lstm.predict(X_test) > 0.5).astype(int)\n",
        "print(\"LSTM Model Accuracy:\", accuracy_score(y_test, y_pred_lstm))\n",
        "print(\"LSTM Model Classification Report:\\n\", classification_report(y_test, y_pred_lstm))\n",
        "print(\"LSTM Model Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lstm))\n",
        "\n",
        "# Model definition - CNN\n",
        "model_cnn = Sequential()\n",
        "model_cnn.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=100))\n",
        "model_cnn.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "model_cnn.add(MaxPooling1D(pool_size=2))\n",
        "model_cnn.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model_cnn.add(MaxPooling1D(pool_size=2))\n",
        "model_cnn.add(Flatten())\n",
        "model_cnn.add(Dense(units=64, activation='relu'))\n",
        "model_cnn.add(Dropout(0.5))\n",
        "model_cnn.add(Dense(units=1, activation='sigmoid'))\n",
        "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model training - CNN\n",
        "history_cnn = model_cnn.fit(X_train, y_train, epochs=8, batch_size=512, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Model evaluation - CNN\n",
        "loss_cnn, accuracy_cnn = model_cnn.evaluate(X_test, y_test)\n",
        "print(f'CNN Test Loss: {loss_cnn}, CNN Test Accuracy: {accuracy_cnn}')\n",
        "y_pred_cnn = (model_cnn.predict(X_test) > 0.5).astype(int)\n",
        "print(\"CNN Model Accuracy:\", accuracy_score(y_test, y_pred_cnn))\n",
        "print(\"CNN Model Classification Report:\\n\", classification_report(y_test, y_pred_cnn))\n",
        "print(\"CNN Model Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_cnn))\n",
        "\n",
        "# Model definition - LSTM-CNN\n",
        "model_lstm_cnn = Sequential()\n",
        "model_lstm_cnn.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "model_lstm_cnn.add(Bidirectional(LSTM(units=200, return_sequences=True)))\n",
        "model_lstm_cnn.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "model_lstm_cnn.add(MaxPooling1D(pool_size=2))\n",
        "model_lstm_cnn.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model_lstm_cnn.add(MaxPooling1D(pool_size=2))\n",
        "model_lstm_cnn.add(Flatten())\n",
        "model_lstm_cnn.add(Dense(units=64, activation='relu'))\n",
        "model_lstm_cnn.add(Dropout(0.5))\n",
        "model_lstm_cnn.add(Dense(units=1, activation='sigmoid'))\n",
        "model_lstm_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model training - LSTM-CNN\n",
        "history_lstm_cnn = model_lstm_cnn.fit(X_train, y_train, epochs=6, batch_size=512, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Model evaluation - LSTM-CNN\n",
        "loss_lstm_cnn, accuracy_lstm_cnn = model_lstm_cnn.evaluate(X_test, y_test)\n",
        "print(f'LSTM-CNN Test Loss: {loss_lstm_cnn}, LSTM-CNN Test Accuracy: {accuracy_lstm_cnn}')\n",
        "y_pred_lstm_cnn = (model_lstm_cnn.predict(X_test) > 0.5).astype(int)\n",
        "print(\"LSTM-CNN Model Accuracy:\", accuracy_score(y_test, y_pred_lstm_cnn))\n",
        "print(\"LSTM-CNN Model Classification Report:\\n\", classification_report(y_test, y_pred_lstm_cnn))\n",
        "print(\"LSTM-CNN Model Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lstm_cnn))\n",
        "\n"
      ],
      "metadata": {
        "id": "dWvHW2jXHq29"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOF3A75b+79Mo6RAXA+HAKb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}