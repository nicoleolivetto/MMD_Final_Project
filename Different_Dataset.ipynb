{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicoleolivetto/MMD_Final_Project/blob/main/Different_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM"
      ],
      "metadata": {
        "id": "VoAf7C2CKUDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "column_names = ['sentiment', 'review']\n",
        "\n",
        "financial_data = pd.read_csv('all-data.csv', names=column_names, encoding='latin1')\n",
        "\n",
        "financial_data.rename(columns={0: 'sentiment', 1: 'review'}, inplace=True)\n",
        "\n",
        "financial_data = financial_data[financial_data['sentiment'] != 'neutral']\n",
        "\n",
        "unique_sentiments = financial_data['sentiment'].unique()\n",
        "\n",
        "print(unique_sentiments)\n",
        "\n",
        "financial_data = financial_data[financial_data['sentiment'] != 'neutral']\n",
        "\n",
        "financial_data=financial_data.sort_index()\n",
        "\n",
        "financial_data = financial_data[['review', 'sentiment']]\n",
        "\n",
        "print(financial_data.columns)\n",
        "\n",
        "\n",
        "positive_count = (financial_data['sentiment'] == 'positive').sum()\n",
        "negative_count = (financial_data['sentiment'] == 'negative').sum()\n",
        "\n",
        "print(\"Number of positive reviews:\", positive_count)\n",
        "print(\"Number of negative reviews:\", negative_count)\n",
        "\n",
        "\n",
        "financial_data['review_length'] = financial_data['review'].apply(len)\n",
        "\n",
        "average_length = financial_data['review_length'].mean()\n",
        "\n",
        "print(\"Average length of reviews:\", average_length)\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Data preprocessing\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopwords_list = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    # Tokenize\n",
        "    tokens = tokenizer.tokenize(text.lower())\n",
        "    # Remove stopwords\n",
        "    tokens = [token for token in tokens if token not in stopwords_list]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "financial_data['clean_review'] = financial_data['review'].apply(preprocess_text)\n",
        "\n",
        "# Tokenization and padding\n",
        "max_length = 100\n",
        "vocab_size = 8000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(financial_data['clean_review'])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(financial_data['clean_review'])\n",
        "X = pad_sequences(sequences, maxlen=max_length)\n",
        "y = np.array(financial_data['sentiment'].map({'positive': 1, 'negative': 0}))\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Model definition\n",
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "model.add(LSTM(units=200))\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model training\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=5, batch_size=8, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Model evaluation\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Evaluation metrics\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7OklZJIJm5Z",
        "outputId": "d3e7c716-17d9-4f54-9197-7a2c3bf34bf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['negative' 'positive']\n",
            "Index(['review', 'sentiment'], dtype='object')\n",
            "Number of positive reviews: 1363\n",
            "Number of negative reviews: 604\n",
            "Average length of reviews: 132.61057447890187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<ipython-input-30-bef9dd132e59>:80: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "138/138 [==============================] - 13s 71ms/step - loss: 0.5881 - accuracy: 0.7155 - val_loss: 0.4775 - val_accuracy: 0.7790\n",
            "Epoch 2/5\n",
            "138/138 [==============================] - 3s 24ms/step - loss: 0.3622 - accuracy: 0.8918 - val_loss: 0.4587 - val_accuracy: 0.7899\n",
            "Epoch 3/5\n",
            "138/138 [==============================] - 3s 21ms/step - loss: 0.1487 - accuracy: 0.9445 - val_loss: 0.4483 - val_accuracy: 0.8043\n",
            "Epoch 4/5\n",
            "138/138 [==============================] - 2s 14ms/step - loss: 0.0990 - accuracy: 0.9636 - val_loss: 0.5953 - val_accuracy: 0.8007\n",
            "Epoch 5/5\n",
            "138/138 [==============================] - 2s 14ms/step - loss: 0.0558 - accuracy: 0.9827 - val_loss: 0.9316 - val_accuracy: 0.8080\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.4565 - accuracy: 0.7851\n",
            "Test Loss: 0.45653823018074036, Test Accuracy: 0.7851099967956543\n",
            "19/19 [==============================] - 0s 4ms/step\n",
            "Accuracy: 0.7851099830795262\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.65      0.66       191\n",
            "           1       0.84      0.85      0.84       400\n",
            "\n",
            "    accuracy                           0.79       591\n",
            "   macro avg       0.75      0.75      0.75       591\n",
            "weighted avg       0.78      0.79      0.78       591\n",
            "\n",
            "[[124  67]\n",
            " [ 60 340]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "D1yKWqmoKFJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
        "\n",
        "# Define the CNN model\n",
        "model_cnn = Sequential()\n",
        "\n",
        "# Embedding layer\n",
        "model_cnn.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=100))\n",
        "\n",
        "# First convolutional layer\n",
        "model_cnn.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "model_cnn.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Second convolutional layer\n",
        "model_cnn.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model_cnn.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Flatten the output of the convolutional layers\n",
        "model_cnn.add(Flatten())\n",
        "\n",
        "# Fully connected layer\n",
        "model_cnn.add(Dense(units=64, activation='relu'))\n",
        "\n",
        "# Dropout layer\n",
        "model_cnn.add(Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model_cnn.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model_cnn.summary()\n",
        "\n",
        "\n",
        "# Training CNN model\n",
        "history_cnn = model_cnn.fit(X_train, y_train, epochs=8, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluation CNN model\n",
        "loss_cnn, accuracy_cnn = model_cnn.evaluate(X_test, y_test)\n",
        "print(f'CNN Test Loss: {loss_cnn}, CNN Test Accuracy: {accuracy_cnn}')\n",
        "y_pred_cnn_prob = model_cnn.predict(X_test)\n",
        "y_pred_cnn = (y_pred_cnn_prob > 0.5).astype(int)\n",
        "\n",
        "# Evaluation metrics for CNN\n",
        "print(\"CNN Model Accuracy:\", accuracy_score(y_test, y_pred_cnn))\n",
        "print(\"CNN Model Classification Report:\\n\", classification_report(y_test, y_pred_cnn))\n",
        "print(\"CNN Model Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_cnn))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V38srzBKGe8",
        "outputId": "5dbd0869-f180-414e-e6dd-8fbc938e8b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_25 (Embedding)    (None, 100, 100)          800000    \n",
            "                                                                 \n",
            " conv1d_28 (Conv1D)          (None, 98, 128)           38528     \n",
            "                                                                 \n",
            " max_pooling1d_28 (MaxPooli  (None, 49, 128)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_29 (Conv1D)          (None, 47, 64)            24640     \n",
            "                                                                 \n",
            " max_pooling1d_29 (MaxPooli  (None, 23, 64)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " flatten_14 (Flatten)        (None, 1472)              0         \n",
            "                                                                 \n",
            " dense_50 (Dense)            (None, 64)                94272     \n",
            "                                                                 \n",
            " dropout_25 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 957505 (3.65 MB)\n",
            "Trainable params: 957505 (3.65 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/8\n",
            "9/9 [==============================] - 3s 191ms/step - loss: 0.6389 - accuracy: 0.6755 - val_loss: 0.6304 - val_accuracy: 0.6775\n",
            "Epoch 2/8\n",
            "9/9 [==============================] - 1s 149ms/step - loss: 0.6097 - accuracy: 0.7055 - val_loss: 0.6253 - val_accuracy: 0.6775\n",
            "Epoch 3/8\n",
            "9/9 [==============================] - 1s 117ms/step - loss: 0.5920 - accuracy: 0.7055 - val_loss: 0.6183 - val_accuracy: 0.6775\n",
            "Epoch 4/8\n",
            "9/9 [==============================] - 1s 112ms/step - loss: 0.5552 - accuracy: 0.7055 - val_loss: 0.5885 - val_accuracy: 0.6775\n",
            "Epoch 5/8\n",
            "9/9 [==============================] - 1s 86ms/step - loss: 0.4377 - accuracy: 0.7055 - val_loss: 0.5589 - val_accuracy: 0.6739\n",
            "Epoch 6/8\n",
            "9/9 [==============================] - 1s 96ms/step - loss: 0.2976 - accuracy: 0.8709 - val_loss: 0.5389 - val_accuracy: 0.7355\n",
            "Epoch 7/8\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 0.1741 - accuracy: 0.9427 - val_loss: 0.9625 - val_accuracy: 0.7138\n",
            "Epoch 8/8\n",
            "9/9 [==============================] - 1s 109ms/step - loss: 0.1372 - accuracy: 0.9545 - val_loss: 1.0874 - val_accuracy: 0.7464\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.5024 - accuracy: 0.7766\n",
            "CNN Test Loss: 0.5024445056915283, CNN Test Accuracy: 0.7766497731208801\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "CNN Model Accuracy: 0.7766497461928934\n",
            "CNN Model Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.49      0.59       191\n",
            "           1       0.79      0.91      0.85       400\n",
            "\n",
            "    accuracy                           0.78       591\n",
            "   macro avg       0.76      0.70      0.72       591\n",
            "weighted avg       0.77      0.78      0.76       591\n",
            "\n",
            "CNN Model Confusion Matrix:\n",
            " [[ 94  97]\n",
            " [ 35 365]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM-CNN"
      ],
      "metadata": {
        "id": "C_gPLIdZKPnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM, Bidirectional\n",
        "\n",
        "# Define the LSTM-CNN model\n",
        "model_lstm_cnn = Sequential()\n",
        "\n",
        "# Embedding layer\n",
        "model_lstm_cnn.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "\n",
        "# LSTM layer\n",
        "model_lstm_cnn.add(Bidirectional(LSTM(units=200, return_sequences=True)))\n",
        "\n",
        "# Convolutional layers\n",
        "model_lstm_cnn.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "model_lstm_cnn.add(MaxPooling1D(pool_size=2))\n",
        "model_lstm_cnn.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model_lstm_cnn.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Flatten the output\n",
        "model_lstm_cnn.add(Flatten())\n",
        "\n",
        "# Fully connected layer\n",
        "model_lstm_cnn.add(Dense(units=64, activation='relu'))\n",
        "\n",
        "# Dropout layer\n",
        "model_lstm_cnn.add(Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model_lstm_cnn.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_lstm_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model_lstm_cnn.summary()\n",
        "\n",
        "# Training LSTM-CNN model\n",
        "history_lstm_cnn = model_lstm_cnn.fit(X_train, y_train, epochs=6, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluation LSTM-CNN model\n",
        "loss_lstm_cnn, accuracy_lstm_cnn = model_lstm_cnn.evaluate(X_test, y_test)\n",
        "print(f'LSTM-CNN Test Loss: {loss_lstm_cnn}, LSTM-CNN Test Accuracy: {accuracy_lstm_cnn}')\n",
        "y_pred_lstm_cnn_prob = model_lstm_cnn.predict(X_test)\n",
        "y_pred_lstm_cnn = (y_pred_lstm_cnn_prob > 0.5).astype(int)\n",
        "\n",
        "# Evaluation metrics for LSTM-CNN\n",
        "print(\"LSTM-CNN Model Accuracy:\", accuracy_score(y_test, y_pred_lstm_cnn))\n",
        "print(\"LSTM-CNN Model Classification Report:\\n\", classification_report(y_test, y_pred_lstm_cnn))\n",
        "print(\"LSTM-CNN Model Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lstm_cnn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNMcJQRnKLD0",
        "outputId": "a65a3fd2-7406-4a0b-9cb5-dfebc0c427ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_26 (Embedding)    (None, 100, 100)          800000    \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 100, 400)          481600    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " conv1d_30 (Conv1D)          (None, 98, 128)           153728    \n",
            "                                                                 \n",
            " max_pooling1d_30 (MaxPooli  (None, 49, 128)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_31 (Conv1D)          (None, 47, 64)            24640     \n",
            "                                                                 \n",
            " max_pooling1d_31 (MaxPooli  (None, 23, 64)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " flatten_15 (Flatten)        (None, 1472)              0         \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 64)                94272     \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1554305 (5.93 MB)\n",
            "Trainable params: 1554305 (5.93 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/6\n",
            "9/9 [==============================] - 7s 233ms/step - loss: 0.6449 - accuracy: 0.6564 - val_loss: 0.6386 - val_accuracy: 0.6775\n",
            "Epoch 2/6\n",
            "9/9 [==============================] - 1s 136ms/step - loss: 0.6247 - accuracy: 0.7055 - val_loss: 0.6463 - val_accuracy: 0.6775\n",
            "Epoch 3/6\n",
            "9/9 [==============================] - 1s 119ms/step - loss: 0.6162 - accuracy: 0.7055 - val_loss: 0.6288 - val_accuracy: 0.6775\n",
            "Epoch 4/6\n",
            "9/9 [==============================] - 1s 133ms/step - loss: 0.5950 - accuracy: 0.7055 - val_loss: 0.5993 - val_accuracy: 0.6775\n",
            "Epoch 5/6\n",
            "9/9 [==============================] - 1s 97ms/step - loss: 0.6137 - accuracy: 0.7127 - val_loss: 0.5606 - val_accuracy: 0.7174\n",
            "Epoch 6/6\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.4032 - accuracy: 0.7618 - val_loss: 0.5938 - val_accuracy: 0.7210\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.5358 - accuracy: 0.7445\n",
            "LSTM-CNN Test Loss: 0.5358175039291382, LSTM-CNN Test Accuracy: 0.7445008754730225\n",
            "19/19 [==============================] - 1s 7ms/step\n",
            "LSTM-CNN Model Accuracy: 0.7445008460236887\n",
            "LSTM-CNN Model Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.32      0.45       191\n",
            "           1       0.75      0.94      0.83       400\n",
            "\n",
            "    accuracy                           0.74       591\n",
            "   macro avg       0.74      0.63      0.64       591\n",
            "weighted avg       0.74      0.74      0.71       591\n",
            "\n",
            "LSTM-CNN Model Confusion Matrix:\n",
            " [[ 62 129]\n",
            " [ 22 378]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyNAi5eqOvdAb048sOSfg//9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}